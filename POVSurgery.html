<!DOCTYPE html>
<html>
<head>
<style>
  .highlight {
    background-color: #0079F2;
    color: #FFF;
    padding: 10px;
    border: 2px solid #0079F2;
    border-radius: 5px;
  }
</style>
    
  <meta charset="utf-8">
  <!-- Meta tags for social media banners, these should be filled in appropriatly as they are your "business card" -->
  <!-- Replace the content tag with appropriate information -->
  
  <meta name="description" content="DESCRIPTION META TAG">
  <meta property="og:title" content="POV-Surgery"/>
  <meta property="og:description" content="A Dataset for Egocentric Hand and Tool Pose Estimation During Surgical Activities"/>
  <meta property="og:url" content="https://batfacewayne.github.io/POV_Surgery_io/"/>
  <!-- Path to banner image, should be in the path listed below. Optimal dimenssions are 1200X630-->
  <meta property="og:image" content="static/image/statistics.png" />
  <meta property="og:image:width" content="1200"/>
  <meta property="og:image:height" content="630"/>


  <meta name="twitter:title" content="POV-Surgery">
  <meta name="twitter:description" content="A Dataset for Egocentric Hand and Tool Pose Estimation During Surgical Activities">
  <!-- Path to banner image, should be in the path listed below. Optimal dimenssions are 1200X600-->
  <meta name="twitter:image" content="static/image/statistics.png">
  <meta name="twitter:card" content="summary_large_image">
  <!-- Keywords for your paper to be indexed by-->
  <meta name="keywords" content="Hand Object Pose Estimation, Deep Learning, Dataset, Mixed Reality">
  <meta name="viewport" content="width=device-width, initial-scale=1">


  <title>POV Surgery</title>
  <link rel="icon" type="image/x-icon" href="static/images/bat.ico">
  <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro"
  rel="stylesheet">

  <link rel="stylesheet" href="static/css/bulma.min.css">
  <link rel="stylesheet" href="static/css/bulma-carousel.min.css">
  <link rel="stylesheet" href="static/css/bulma-slider.min.css">
  <link rel="stylesheet" href="static/css/fontawesome.all.min.css">
  <link rel="stylesheet"
  href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
  <link rel="stylesheet" href="static/css/index.css">

  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
  <script src="https://documentcloud.adobe.com/view-sdk/main.js"></script>
  <script defer src="static/js/fontawesome.all.min.js"></script>
  <script src="static/js/bulma-carousel.min.js"></script>
  <script src="static/js/bulma-slider.min.js"></script>
  <script src="static/js/index.js"></script>
</head>
<body>


  <section class="hero">
    <div class="hero-body">
      <div class="container is-max-desktop">
        <div class="columns is-centered">
          <div class="column has-text-centered">
            <h1 class="title is-1 publication-title">POV-Surgery: A Dataset for Egocentric Hand and Tool Pose Estimation During Surgical Activities</h1>
            <div class="is-size-5 publication-authors">
              <!-- Paper authors -->
              <span class="author-block">
                <a href="FIRST AUTHOR PERSONAL LINK" target="_blank">Rui Wang</a><sup>*</sup>,</span>
                <span class="author-block">
                  <a href="SECOND AUTHOR PERSONAL LINK" target="_blank">Sophokles Ktistakis</a><sup>*</sup>,</span>
                                  <span class="author-block">
                  <a href="SECOND AUTHOR PERSONAL LINK" target="_blank">Siwei Zhang</a></sup>,</span>
                  
                                   <span class="author-block">
                  <a href="SECOND AUTHOR PERSONAL LINK" target="_blank">Mirko Meboldt</a></sup>,</span>                 
                  
                  <span class="author-block">
                    <a href="THIRD AUTHOR PERSONAL LINK" target="_blank">Quentin Lohmeyer</a>
                  </span>
                  </div>

                  <div class="is-size-5 publication-authors">
<span class="author-block">ETH Zurich<br>26th International Conference on Medical Image Computing and Computer Assisted Intervention<br> (MICCAI) 2023 &nbsp; <span class="highlight">Oral Presentation</span></span>


                    </span>
                    <span class="eql-cntrb"><small><br><sup>*</sup>Indicates co-first authorship</small></span>
                  </div>

                  <div class="column has-text-centered">
                    <div class="publication-links">
                         <!-- Arxiv PDF link -->
                      <span class="link-block">
                        <a href="https://arxiv.org/abs/2307.10387" target="_blank"
                        class="external-link button is-normal is-rounded is-dark">
                        <span class="icon">
                          <i class="fas fa-file-pdf"></i>
                        </span>
                        <span>Paper</span>
                      </a>
                    </span>

                    <!-- Supplementary PDF link -->
                    <span class="link-block">
                      <a href="static/pdfs/supplementary_material.pdf" target="_blank"
                      class="external-link button is-normal is-rounded is-dark">
                      <span class="icon">
                        <i class="fas fa-database"></i>
                      </span>
                      <span>data</span>
                    </a>
                  </span>

                  <!-- Github link -->
                  <span class="link-block">
                    <a href="https://github.com/YOUR REPO HERE" target="_blank"
                    class="external-link button is-normal is-rounded is-dark">
                    <span class="icon">
                      <i class="fab fa-github"></i>
                    </span>
                    <span>Code</span>
                  </a>
                </span>

                <!-- ArXiv abstract Link -->
                <span class="link-block">
                  <a href="https://arxiv.org/abs/2307.10387" target="_blank"
                  class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                    <i class="ai ai-arxiv"></i>
                  </span>
                  <span>arXiv</span>
                </a>
              </span>
            </div>
          </div>
        </div>
      </div>
    </div>
  </div>
</section>
<!-- Youtube video -->
<section class="hero is-small is-light">
  <div class="hero-body">
    <div class="container">
      <!-- Paper video. -->
      
      <div class="columns is-centered has-text-centered">
        <div class="column is-four-fifths">
          
          <div class="publication-video">
            <!-- Youtube embed code here -->
            <iframe src="https://www.youtube.com/embed/lA0FI83Y9tg" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe>
          </div>
        </div>
      </div>

    </div>
          <h2 class="subtitle has-text-centered">  
   Overview of the proposed POV-Surgery synthetic dataset. 
      </h2>
  </div>
</section>
<!-- End youtube video -->



<!-- Paper abstract -->
<section class="section hero is-light">
  <div class="container is-max-desktop">
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Abstract</h2>
        <div class="content has-text-justified">
          <p>
          The surgical usage of Mixed Reality (MR) has received growing attention in areas such as surgical navigation systems, skill assessment, and robot-assisted surgeries. For such applications, pose estimation for hand and surgical instruments from an egocentric perspective is a fundamental task and has been studied extensively in the computer vision field in recent years. However, the development of this field has been impeded by a lack of datasets, especially in the surgical field, where bloody gloves and reflective metallic tools make it hard to obtain 3D pose annotations for hands and objects using conventional methods. To address this issue, we propose POV-Surgery, a large-scale, synthetic, egocentric dataset focusing on pose estimation for hands with different surgical gloves and three orthopedic surgical instruments, namely scalpel, friem, and diskplacer. Our dataset consists of 53 sequences and 88,329 frames, featuring high-resolution RGB-D video streams with activity annotations, accurate 3D and 2D annotations for hand-object pose, and 2D hand-object segmentation masks. We fine-tune the current SOTA methods on POV-Surgery and further show the generalizability when applying to real-life cases with surgical gloves and tools by extensive evaluations. The code and the dataset will be publicly available for research purposes.
          </p>
        </div>
      </div>
    </div>
  </div>
</section>
<!-- End paper abstract -->


<!-- Image carousel -->
<section class="hero is-small">
  <div class="hero-body">
    <div class="container">
      <div id="results-carousel" class="carousel results-carousel">
       <div class="item">
        <!-- Your image here -->
        <img src="static/images/pipeline.jpg" alt="MY ALT TEXT"/>
        <h2 class="subtitle has-text-centered">
          The proposed pipeline to generate synthetic data sequences. (a) shows the multi-stereo-cameras-based body motion capture module. (b) shows the optimization- based hand-object manipulation sequence generation pipeline. (c) shows the fused hand-body pose and the egocentric camera pose calculation module. (d) shows the rendering module with which RGB-D sequences are rendered with diverse textures.
        </h2>
      </div>
      <div class="item">
        <!-- Your image here -->
        <img src="static/images/posegenv2.jpg" alt="MY ALT TEXT"/>
        <h2 class="subtitle has-text-centered">
          The Hand manipulation sequence generation pipeline consists of three components: grasp pose generation, pose selection, and pose refinement, highlighted in blue, red, and green, respectively.
        </h2>
      </div>
      <div class="item">
        <!-- Your image here -->
        <img src="static/images/statistics.png" alt="MY ALT TEXT"/>
        <h2 class="subtitle has-text-centered">
         (a) Dataset samples for RGB-D sequences and annotation. An example of the scalpel, friem, and diskplacer, is shown in the first three rows. The fourth row shows an example of the new scene and blood glove patterns that only appear in the test set. (b) shows the statistics on the number of frames for each surgical instrument in the training and testing sets. (c) shows a point cloud created from an RGB-D frame with simulated Kinect noise.
         
       </h2>
            </div>
      <div class="item">
        <!-- Your image here -->
        <img src="static/images/quali.png" alt="MY ALT TEXT"/>
        <h2 class="subtitle has-text-centered">
         Qualitative results of METRO, SEMI, and HANDOCCNET on the test of of POV-Surgery. The FT denotes fine-tuning. We show the 2D re-projection of the predicted 3D hand joints and object control bounding box overlayed on the input image.         
       </h2>
             </div>
      <div class="item">
        <!-- Your image here -->
        <img src="static/images/quanti.png" alt="MY ALT TEXT"/>
        <h2 class="subtitle has-text-centered">
         The evaluation result of different methods on the test set of POV-Surgery, where the ft denotes fine-tuned on the training set. P2d denotes the 2D hand joint re-projection error (in pixels). MPJPE and PVE denote the 3D mean per joint error and per hand vertex error, respectively. PA denotes procrustes alignment.         
       </h2>
     </div>
     <div class="item">
      <!-- Your image here -->
      <img src="static/images/real.png" alt="MY ALT TEXT"/>
      <h2 class="subtitle has-text-centered">
        (a) Ground truth and qualitative results of different methods on the real-life test set. (b) Accuracy with different 2D pixel error thresholds, showing large performance improvement after fine-tuning on POV-Surgery (c) Our multi-camera real-life data capturing set-up.
      </h2>
    </div>
  </div>
</div>
</div>
</section>
<!-- End image carousel -->


<!-- Paper poster -->
<section class="hero is-small is-light">
  <div class="hero-body">
    <div class="container" style="display: flex; flex-direction: column; align-items: center; justify-content: center;">
      <h2 class="title">Poster</h2>

      <iframe src="static/pdfs/poster.pdf" width="70%" height="550"></iframe>        
    </div>
  </div>
</section>
<!--End paper poster -->



<!--BibTex citation -->
  <section class="section" id="BibTeX">
    <div class="container is-max-desktop content">
      <h2 class="title">BibTeX</h2>
      <pre><code>@misc{wang2023povsurgery,
      title={POV-Surgery: A Dataset for Egocentric Hand and Tool Pose Estimation During Surgical Activities}, 
      author={Rui Wang and Sophokles Ktistakis and Siwei Zhang and Mirko Meboldt and Quentin Lohmeyer},
      year={2023},
      eprint={2307.10387},
      archivePrefix={arXiv},
      primaryClass={cs.CV}
}</code></pre>
    </div>
</section>

<!--End BibTex citation -->


  <footer class="footer">
  <div class="container">
    <div class="columns is-centered">
      <div class="column is-8">
        <div class="content">

          <p>
          										  Templates cr. to <a href="https://github.com/eliahuhorwitz/Academic-project-page-template" target="_blank">eliahuhorwitz</a>.

          </p>

</footer>

<!-- Statcounter tracking code -->
  
<!-- You can add a tracker to track page visits by creating an account at statcounter.com -->

    <!-- End of Statcounter Code -->

  </body>
  </html>
